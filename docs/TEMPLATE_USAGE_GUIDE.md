# LAP Pipeline Template Usage Guide

This guide walks you through using this template repository to jumpstart your LAP pipeline development.

## Quick Start

### 1. Clone and Set Up

```bash
# Clone the template repository
git clone <template-repo-url> my-lap-project
cd my-lap-project

# Install dependencies with uv (much faster and simpler than venv/pip)
uv sync
```

### 2. Configure Your Environment

Edit the paths in `config/starter.meta.yaml` to match your system:

```bash
cp config/starter.meta.yaml config/my-project.meta.yaml
```

Edit `config/my-project.meta.yaml`:
```yaml
keys:
  # Update these paths for your system
  base_dir: "/path/to/your/project/directory"
  unix_out_dir: "${base_dir}/out"
  log_dir: "${base_dir}/log"
  lap_home: "/path/to/your/lap/installation"

classes:
  my_analysis_project:
    class: project
    parent: null
```

### 3. Generate LAP Meta File and Test

```bash
# Generate traditional LAP meta file from YAML
meta-sanity generate-meta config/my-project.meta.yaml config/my-project.meta

# Check that LAP can parse your configuration
perl lap-src/trunk/bin/run.pl --meta config/my-project.meta --check

# Initialize the pipeline (creates directories)
perl lap-src/trunk/bin/run.pl --meta config/my-project.meta --init
```

## Project Structure Overview

After cloning, you'll have this structure:

```
my-lap-project/
â”œâ”€â”€ config/                      # Pipeline configurations
â”‚   â”œâ”€â”€ examples/                # Example config/meta files
â”‚   â”‚   â”œâ”€â”€ chase/               # Chase's clean examples
â”‚   â”‚   â””â”€â”€ jason/               # Jason's advanced examples
â”‚   â”œâ”€â”€ starter.cfg              # Template configuration file
â”‚   â””â”€â”€ starter.meta.yaml        # Template meta file (YAML format)
â”œâ”€â”€ src/                         # Your development code
â”‚   â”œâ”€â”€ workflows/               # Workflow scripts (one per LAP class)
â”‚   â””â”€â”€ modules/                 # Reusable utility modules
â”œâ”€â”€ raw/                         # Raw input data files
â”œâ”€â”€ out/                         # Pipeline output files (created by LAP)
â”œâ”€â”€ log/                         # Pipeline log files (created by LAP)
â”œâ”€â”€ docs/                        # Documentation
â”œâ”€â”€ examples/                    # Reference examples from LAP
â”œâ”€â”€ lap-src/                     # LAP source code
â”œâ”€â”€ pyproject.toml               # Python dependencies and project config
â”œâ”€â”€ uv.lock                      # Dependency lock file (generated by uv)
â””â”€â”€ README.md
```

## âš¡ UV Python Environment Management

This template uses **uv** for fast, reliable Python dependency management instead of traditional pip/venv workflows.

### Key Benefits:
- **10-100x faster** than pip for dependency resolution and installation
- **Automatic virtual environment management** - no need to create/activate environments manually
- **Reproducible builds** via `uv.lock` lockfile
- **Simplified workflow** - just run `uv sync` to set up everything

### UV Commands:

```bash
# Initial setup (installs all dependencies, creates venv automatically)
uv sync

# Add new dependencies
uv add pandas matplotlib seaborn

# Add optional dependency groups
uv sync --group plotting    # Install plotting dependencies
uv sync --group statistics  # Install statistical analysis tools
uv sync --group all        # Install all optional dependencies

# Run Python scripts (automatically uses project environment)
uv run python script.py
uv run src/workflows/analysis_workflow.py qc --input data.tsv --output qc.json

# Development tools
uv sync --group dev         # Install development tools (pytest, black, etc.)
uv run pytest tests/       # Run tests
uv run black src/          # Format code
```

### LAP Integration:
The template's `starter.cfg` defines `run_uv_cmd=uv run --project $project_root` for seamless integration:

```cfg
# LAP commands automatically use uv for Python execution
local cmd analysis_cmd=$run_uv_cmd src/workflows/analysis_workflow.py analyze \
    --input !{input::data} --output !{output::results}
```

## Development Workflow

### Step 1: Design Your Pipeline

1. **Identify your analysis steps**: What are the major computational phases?
2. **Define your data flow**: What files flow between steps?
3. **Plan your class hierarchy**: How should your pipeline be organized?

Example analysis planning:
```
Raw Data â†’ Quality Control â†’ Statistical Analysis â†’ Visualization â†’ Report Generation
```

### Step 2: Set Up Configuration Files

#### Create Your Config File

Start with `config/starter.cfg` and modify for your needs:

```cfg
# Copy and customize the starter config
cp config/starter.cfg config/my-analysis.cfg
```

Edit the config file to add your classes:
```cfg
# Define your analysis classes
class project=Project
class dataset=Dataset parent project
class analysis=Analysis parent dataset
class result=Result parent analysis
```

#### Create Your Meta YAML File

Define your specific instances using the much more readable YAML format:

```bash
# Copy and customize
cp config/starter.meta.yaml config/my-analysis.meta.yaml
```

Edit to add your instances:
```yaml
config: "config/my-analysis.cfg"

keys:
  base_dir: "/path/to/your/project"
  unix_out_dir: "${base_dir}/out"
  log_dir: "${base_dir}/log"

classes:
  genomics_study:
    class: project
    parent: null

  gwas_data:
    class: dataset
    parent: genomics_study
    properties:
      input_file: "/data/gwas.tsv"

  association_test:
    class: analysis
    parent: gwas_data
```

Then generate the traditional LAP meta file:
```bash
meta-sanity generate-meta config/my-analysis.meta.yaml config/my-analysis.meta
```

### Step 3: Develop Workflows

For each LAP class, create a corresponding workflow in `src/workflows/`:

```bash
# Create a workflow for your 'analysis' class
touch src/workflows/analysis_workflow.py
chmod +x src/workflows/analysis_workflow.py
```

Use the template structure from the [Workflow Development Guide](WORKFLOW_DEVELOPMENT_GUIDE.md):

```python
#!/usr/bin/env python3
"""Analysis Workflow for My LAP Pipeline"""

import argparse
import logging
import sys
from pathlib import Path

def handle_stage_qc(args):
    """Quality control stage"""
    logging.info("Running quality control")
    # Your QC logic here

def handle_stage_analysis(args):
    """Main analysis stage"""
    logging.info("Running main analysis")
    # Your analysis logic here

def main():
    parser = argparse.ArgumentParser(
        prog='analysis_workflow',
        description='Analysis workflow for genomics pipeline'
    )

    subparsers = parser.add_subparsers(dest='command', required=True)

    # QC stage
    parser_qc = subparsers.add_parser('qc', help='Quality control')
    parser_qc.add_argument('--input-file', required=True)
    parser_qc.add_argument('--output-file', required=True)

    # Analysis stage
    parser_analysis = subparsers.add_parser('analysis', help='Main analysis')
    parser_analysis.add_argument('--input-file', required=True)
    parser_analysis.add_argument('--output-file', required=True)

    args = parser.parse_args()

    if args.command == 'qc':
        handle_stage_qc(args)
    elif args.command == 'analysis':
        handle_stage_analysis(args)

if __name__ == '__main__':
    main()
```

### Step 4: Add LAP Commands

In your config file, add commands that call your workflows:

```cfg
# Add commands for each workflow stage (using uv for dependency management)
local cmd quality_control_cmd=$run_uv_cmd src/workflows/analysis_workflow.py qc \
    --input-file !{input::dataset_file} \
    --output-file !{output::qc_results_file}; \
    class_level analysis

local cmd main_analysis_cmd=$run_uv_cmd src/workflows/analysis_workflow.py analysis \
    --input-file !{input::qc_results_file} \
    --output-file !{output::analysis_results_file}; \
    class_level analysis
```

### Step 5: Define Files and Directories

Add file definitions to your config:

```cfg
# Directories
sortable mkdir path project_dir=$unix_out_dir/projects/@project class_level project
sortable mkdir path dataset_dir=$project_dir/datasets/@dataset class_level dataset
sortable mkdir path analysis_dir=$dataset_dir/analyses/@analysis class_level analysis

# Files
path file dataset_file=@dataset.input_file dir dataset_dir disp "Raw Dataset" class_level dataset
path file qc_results_file=@analysis.qc_results.tsv dir analysis_dir disp "QC Results" class_level analysis
path file analysis_results_file=@analysis.results.tsv dir analysis_dir disp "Analysis Results" class_level analysis
```

### Step 6: Test and Iterate

```bash
# Regenerate meta file after YAML changes
meta-sanity generate-meta config/my-analysis.meta.yaml config/my-analysis.meta

# Check your configuration
perl lap-src/trunk/bin/run.pl --meta config/my-analysis.meta --check

# Initialize (create directories)
perl lap-src/trunk/bin/run.pl --meta config/my-analysis.meta --init

# Run the pipeline
perl lap-src/trunk/bin/run.pl --meta config/my-analysis.meta
```

## ðŸ”„ Meta-Sanity Advanced Features

Meta-sanity provides powerful templating capabilities that make managing large numbers of instances much easier than traditional LAP meta files.

### Template Types

#### 1. For Each Item Template
Generate one instance per item in a list:

```yaml
templates:
  trait_analyses:
    class: analysis
    operation: for_each_item
    input:
      - BMI
      - HEIGHT
      - T2D
      - HDL
    pattern:
      name: "trait_${item}"
      parent: main_study
      properties:
        trait_name: "${item}"
        gwas_file: "/data/${item}.sumstats.gz"
```

This generates: `trait_BMI`, `trait_HEIGHT`, `trait_T2D`, `trait_HDL`

#### 2. Combinatorial Template
Generate instances for all combinations of multiple factors:

```yaml
templates:
  method_dataset_combinations:
    class: analysis
    operation: iter.combination
    input:
      - name: method
        values: ["linear", "logistic", "survival"]
      - name: cohort
        values: ["discovery", "replication"]
    pattern:
      name: "${item:method}_${item:cohort}"
      parent: main_study
      properties:
        analysis_method: "${item:method}"
        cohort_type: "${item:cohort}"
```

This generates: `linear_discovery`, `linear_replication`, `logistic_discovery`, etc.

#### 3. Class-based Template
Generate instances based on existing class instances:

```yaml
templates:
  per_trait_results:
    class: result
    operation: for_each_class
    input:
      class_name: analysis
      if_property: "trait_name"  # Only analyses with trait_name property
    pattern:
      name: "results_${item}"
      parent: "${item}"  # Parent is the analysis instance itself
```

### Variable Substitution

Meta-sanity supports flexible variable substitution:

```yaml
keys:
  base_dir: "/project/data"
  gwas_dir: "${base_dir}/gwas"
  results_dir: "${base_dir}/results"

classes:
  my_analysis:
    class: analysis
    properties:
      input_gwas: "${gwas_dir}/trait.sumstats.gz"
      output_results: "${results_dir}/analysis_results.tsv"
```

### Validation and Documentation

Meta-sanity validates your YAML structure and provides helpful error messages:

```bash
# Validate YAML structure before generating
meta-sanity validate config/my-project.meta.yaml

# Generate with verbose output
meta-sanity generate-meta config/my-project.meta.yaml config/my-project.meta --verbose
```

## Common Customization Patterns

### Adding New Data Types

To add support for a new file type (e.g., VCF files):

1. **Add file definition in config**:
```cfg
path file vcf_file=@dataset.variants.vcf.gz dir dataset_dir disp "VCF File" class_level dataset
```

2. **Add processing workflow**:
```python
def handle_stage_vcf_processing(args):
    """Process VCF file"""
    # VCF processing logic
```

3. **Add LAP command**:
```cfg
local cmd process_vcf_cmd=python src/workflows/vcf_workflow.py process \
    --input-vcf !{input::vcf_file} \
    --output-tsv !{output::processed_variants_file}; \
    class_level dataset
```

### Adding External Tools

To integrate external command-line tools:

1. **Create wrapper workflow**:
```python
def handle_stage_plink_analysis(args):
    """Run PLINK analysis"""
    cmd = f"plink --bfile {args.input_prefix} --assoc --out {args.output_prefix}"
    subprocess.run(cmd, shell=True, check=True)
```

2. **Add to LAP config**:
```cfg
local cmd plink_analysis_cmd=$run_uv_cmd src/workflows/plink_workflow.py run-plink \
    --input-prefix !{input::plink_prefix} \
    --output-prefix !{output::plink_results_prefix}; \
    class_level analysis
```

### Handling Large-Scale Parallelization

For analyses that process many samples/traits in parallel, use meta-sanity templates:

```yaml
templates:
  # Process hundreds of traits automatically
  trait_analyses:
    class: analysis
    operation: for_each_item
    input:
      - trait_001
      - trait_002
      - trait_003
      # ... can easily add hundreds more
    pattern:
      name: "${item}_analysis"
      parent: gwas_data
      properties:
        trait_file: "/data/${item}.tsv"
        trait_name: "${item}"

  # Or load from external file
  trait_analyses_from_file:
    class: analysis
    operation: for_each_item
    input_file: "config/trait_list.txt"  # One trait per line
    pattern:
      name: "${item}_analysis"
      parent: gwas_data
      properties:
        trait_file: "/data/traits/${item}.sumstats.gz"
```

This approach is much more maintainable than manually defining hundreds of instances in traditional meta files.

## Advanced Usage

### Development Workflow with Meta-Sanity

The recommended development workflow emphasizes the YAML format:

```bash
# 1. Always edit the YAML file (never edit .meta directly)
nano config/my-project.meta.yaml

# 2. Regenerate LAP meta file after any changes
meta-sanity generate-meta config/my-project.meta.yaml config/my-project.meta

# 3. Test LAP configuration
perl lap-src/trunk/bin/run.pl --meta config/my-project.meta --check

# 4. Run pipeline
perl lap-src/trunk/bin/run.pl --meta config/my-project.meta
```

### Tips for Large Projects

1. **Keep YAML files in version control**, not the generated .meta files
2. **Use templates extensively** to avoid repetitive instance definitions
3. **Organize complex projects** with multiple YAML files:
   ```bash
   config/
   â”œâ”€â”€ base.meta.yaml           # Core project structure
   â”œâ”€â”€ traits.meta.yaml         # Trait-specific instances
   â”œâ”€â”€ methods.meta.yaml        # Method configurations
   â””â”€â”€ production.meta.yaml     # Production settings
   ```
4. **Use validation frequently** during development:
   ```bash
   meta-sanity validate config/*.meta.yaml
   ```

### Environment Management

With `uv`, environment management is automatic and reproducible:

```cfg
# UV handles environment management automatically
local cmd analysis_cmd=$run_uv_cmd src/workflows/analysis_workflow.py analysis \
    --input !{input::data_file} \
    --output !{output::results_file}; \
    class_level analysis
```

Benefits of using `uv`:
- **No manual environment activation needed** - `uv run` handles this automatically
- **Reproducible environments** via `uv.lock` file
- **Faster dependency resolution** than pip/conda
- **Project isolation** without manual venv management

### Resource Management

Configure compute resources in your config:

```cfg
# Override compute settings
max_jobs=100
default_mem=8000
bsub_opts=-pe smp 4

# Command-specific resource requirements
short cmd quick_analysis_cmd=...  # Uses default resources
long cmd intensive_analysis_cmd=...  # Uses more time/memory
```

## Troubleshooting

### Common Issues

1. **File not found errors**:
   - Check file paths in meta file
   - Verify LAP directory creation with `--init`
   - Check file permissions

2. **Command not found errors**:
   - Verify workflow script permissions (`chmod +x`)
   - Check Python environment activation
   - Verify paths to external tools

3. **Dependency resolution issues**:
   - Use `--check` flag to validate config
   - Review file input/output dependencies
   - Check for circular dependencies

### Debugging Tips

1. **Use verbose logging**:
```bash
perl lap-src/trunk/bin/run.pl --meta config/my-analysis.meta --verbose
```

2. **Run individual commands**:
```bash
uv run src/workflows/analysis_workflow.py qc --help
uv run src/workflows/analysis_workflow.py qc --input test.tsv --output test_qc.tsv
```

3. **Check LAP web UI** (if configured):
```
https://your-server/~user/lap/
```

## Next Steps

1. **Study the examples**: Review configurations in `examples/chase/` and `examples/jason/`
2. **Read the documentation**: See `docs/LAP_SYNTAX_GUIDE.md` for detailed syntax
3. **Follow best practices**: See `docs/WORKFLOW_DEVELOPMENT_GUIDE.md` for development patterns
4. **Customize for your domain**: Adapt the template to your specific analysis needs

This template provides a solid foundation for LAP pipeline development. Focus on creating atomic, testable workflow stages and let LAP handle the orchestration and dependency management.